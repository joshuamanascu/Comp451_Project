{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8F5l2-kJ1HG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzrQBeSbJ1HI"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "#cheaters = np.load(\"drive/MyDrive/Comp 451 Final Project/data/cheaters.npy\")\n",
        "#clean = np.load(\"drive/MyDrive/Comp 451 Final Project/data/legit.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbzPT0atI30M"
      },
      "outputs": [],
      "source": [
        "cheaters = np.load(\"data/cheaters.npy\")\n",
        "clean = np.load(\"data/legit.npy\")\n",
        "\n",
        "#Upsample cheaters\n",
        "cheaters = np.repeat(cheaters, 5, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RZZoaMBJ1HI"
      },
      "outputs": [],
      "source": [
        "#Create labels for both\n",
        "cheaters_labels = np.ones(10000, dtype=np.float32)\n",
        "clean_labels = np.zeros(10000, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFO-4ra0J1HI"
      },
      "outputs": [],
      "source": [
        "#Create combined data and labels arrays\n",
        "\n",
        "x = np.concatenate((cheaters, clean))\n",
        "y = np.concatenate((cheaters_labels, clean_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c42YH3DTJ1HJ"
      },
      "outputs": [],
      "source": [
        "#Create training, validation, and testing sets\n",
        "\n",
        "#20% for test set\n",
        "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, stratify=y, random_state=17)\n",
        "\n",
        "#20% for validation set, 60% for training\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgG2jBbjJ1HJ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = torch.tensor(x)\n",
        "        self.y = torch.tensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUACmGOLeaYt"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2TJ6D91J1HJ"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(x_train, y_train)\n",
        "validation_dataset = CustomDataset(x_val, y_val)\n",
        "test_dataset = CustomDataset(x_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh8DzBoZeO7P"
      },
      "outputs": [],
      "source": [
        "#CNN - Feature Extractor\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        #Input = (1, 30, 192, 5)\n",
        "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding='same')\n",
        "\n",
        "        self.conv2 = nn.Conv3d(16, 64, kernel_size=3, padding='same')\n",
        "\n",
        "        #Reduce in half\n",
        "        self.pool = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "        #(64, 15, 96, 2)\n",
        "        self.fc1 = nn.Linear(64*15*96*2, 1024)\n",
        "        self.fc2 = nn.Linear(1024,512)\n",
        "        self.fc3 = nn.Linear(512,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Add a dimension for channel\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        features = F.relu(self.fc2(x))\n",
        "        x = self.fc3(features)\n",
        "\n",
        "        return features, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKym9_GpwfDe"
      },
      "outputs": [],
      "source": [
        "model = Net()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZJZDFzpgOHe",
        "outputId": "8d3d09ea-28c2-4c99-925d-354c61defc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9503542151451111\n",
            "0.6940063591003418\n",
            "0.50388480981191\n",
            "0.4509412086009979\n",
            "0.2615954910318057\n",
            "0.3577180780172348\n",
            "0.09160578254858653\n",
            "0.22154358583688735\n",
            "0.03954578260580699\n",
            "0.26642551746964455\n",
            "0.01855705252289772\n",
            "0.28914263350516556\n",
            "0.021648334998249388\n",
            "0.3173749392777681\n",
            "0.010690175398020073\n",
            "0.2197580417599529\n",
            "0.003954579912940972\n",
            "0.29578026963304727\n",
            "0.0034338398527373405\n",
            "0.27652486599050463\n",
            "Best Validation Loss: 0.2198\n"
          ]
        }
      ],
      "source": [
        "#CNN MODEL TRAINING\n",
        "\n",
        "best_validation_loss = np.inf\n",
        "best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    training_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    #TRAINING\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        _, outputs = model(inputs)\n",
        "        outputs = outputs.squeeze()\n",
        "\n",
        "        #print(outputs)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        training_loss += loss.item()\n",
        "\n",
        "\n",
        "    #VALIDATION\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            _, outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "    validation_loss = validation_loss / len(validation_loader)\n",
        "\n",
        "    scheduler.step(validation_loss)\n",
        "\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(training_loss / len(train_loader))\n",
        "    print(validation_loss)\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(best_model)\n",
        "print(f'Best Validation Loss: {best_validation_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s94_FhlK0npd"
      },
      "outputs": [],
      "source": [
        "#Getting features from CNN.\n",
        "#We extract features from the entire dataset, as it will not be updating weights at this stage\n",
        "#The CNN has never used the test data to update its weights\n",
        "\n",
        "rf_x_train = np.concatenate((x_train, x_val))\n",
        "rf_y_train = np.concatenate((y_train, y_val))\n",
        "\n",
        "rf_train_dataset = CustomDataset(rf_x_train, rf_y_train)\n",
        "rf_dataloader = DataLoader(rf_train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "features_list_train = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in rf_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        features, _ = model(inputs)\n",
        "        features.squeeze()\n",
        "        features_list_train.append(features.cpu().numpy())\n",
        "\n",
        "features_list_train = np.vstack(features_list_train) #Flattening\n",
        "\n",
        "features_list_test = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        features, _ = model(inputs)\n",
        "        features.squeeze()\n",
        "        features_list_test.append(features.cpu().numpy())\n",
        "\n",
        "features_list_test = np.vstack(features_list_test) #Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2whwy5wI30P",
        "outputId": "8219c62b-e1e1-4306-f438-cdf07b39e7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Performance:\n",
            "Accuracy: 97.12%\n",
            "Precision: 0.9456\n"
          ]
        }
      ],
      "source": [
        "#The random forest is fit based on the features extracted by the CNN\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=17, class_weight='balanced')\n",
        "rf.fit(features_list_train, rf_y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf.predict(features_list_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Random Forest Performance:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF27npTCI30P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}